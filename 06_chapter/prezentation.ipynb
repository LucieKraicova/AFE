{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 6. Kernel Smoothing Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from interactive_plot import interact_plot_one, interact_plot_two\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Local Regression** \n",
    "    - 6.1 Local Regression in $\\mathbb{R}^1$\n",
    "    - 6.3 Local Regression in $\\mathbb{R}^p$\n",
    "    - 6.4 Structured Local Regression in $\\mathbb{R}^p$\n",
    "    - 6.5 Local Likelyhood and Other Models\n",
    "- 6.6 **Kernel Density Estimation and Classification**\n",
    "- 6.7 Radial Basis Functions and Kernels\n",
    "- 6.8 Mixture Models for Density Estimation and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Local regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "- **general regression setting**: \n",
    "    - estimating the regression function $f(X)$ over the domain $\\mathbb{R}^p$\n",
    "- **Linear regression** : model is the same for all data points \n",
    "    - TOO RESTRICTIVE?\n",
    "    - what if the true relationship cannot be explained by a single model?\n",
    "- **K nearest neigbours** : different model for each data point \n",
    "    - NOT RESTRICTIVE ENOUGH?\n",
    "    - it may be reasonable to assume the true relationhip is smooth\n",
    "  \n",
    "### Question:  \n",
    " - **Why is the k nearest neighbours function not smooth?**\n",
    "     - (Chart: increase k, move indx_x0)\n",
    "     - moving window: points jump in and out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a0f8eece5e4affb2ea018de565daed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='tricube', description='right'), Text(value='knn', description='left'), IntSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Motivation of local regression\n",
    "interact_plot_two(right=\"tricube\",lmbda_right = fixed(5),\n",
    "                  k_left = widgets.IntSlider(min=1,max=100,step=1,value=1),\n",
    "                  pol_order_right=fixed(2),left=\"knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a kernel? \n",
    "- weighting function $K_\\lambda(x_0,x_i)$, which assigns a weight to $x_i$ based on its distance from $x_0$.\n",
    "\n",
    "### How does local regression fit into the spectrum of regression methods?\n",
    "- complexity restriction: \n",
    "    - define neighbourhood - explicitely controlled by widnow width and kernel type \n",
    "    - define behavior on that neighbourhood - constant, linear, polynomial \n",
    "- **model**: training data set, different at each point $x_0$.\n",
    "- similar to k nearest neighbouts, but with weights\n",
    "- **parameter** estimated from data: $\\lambda$\n",
    "- **estimated function** $\\hat{f}(X)$ is smooth in $\\mathbb{R}^p$\n",
    "- local regression estimate of $f(x_0)$ as $f_{\\hat{\\theta}}(x_0)$, where $\\hat{\\theta}$ minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f_\\theta,x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)\\left(y_i -f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "### What do we need to choose?\n",
    "- kernel\n",
    "- window width \n",
    "- polynomial order\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us start with polynomial order 0 (constant fit). \n",
    "K nearest neighbours\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\text{Ave}\\left( y_i|x_i\\in N_k(x_0) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Nadaraya-Watson weighted average\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac{\\sum_{i=1}^N K_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the estimation work? & Selecting window width\n",
    "\n",
    "There is bias-variance tradeoff in selecting the window width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb5a9d1164c49a48c0844bc21253002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='tricube', description='right'), Text(value='tricube', description='left'), F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nadaraya-Watson kernel weighted average\n",
    "interact_plot_two(right=\"tricube\",left=\"tricube\",k_left = fixed(10),k_right = fixed(10), show_estimated_y = fixed(True),\n",
    "                 pol_order_left = fixed(0),pol_order_right = fixed(0))\n",
    "                  \n",
    "# Use chart to explain the estimation: slide idx_x0\n",
    "# --------------------------------\n",
    "# just like with k nearest neighbours:  for each point different model\n",
    "# kernel is used as weights in the weighted average\n",
    "# blue points are assigned 0 weight\n",
    "\n",
    "# Selecting the window width : slide lambda left, lambda right\n",
    "# ------------------------------ \n",
    "# Lessons:\n",
    "# higher lambda resembles global fit\n",
    "# lower lambda resembles k neareast neighbours\n",
    "\n",
    "# Illustrate bias-variance tradeoff in selecting window width : slide seed\n",
    "# -------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with constant fit: bias at boundaries, and of first, second, and higher polynomial order\n",
    "### Problem with linear fit: bias in second and higher polynomial order\n",
    "\n",
    "The bias can be resolved by introducing polynomial fits, but this comes at cost of increased variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5598de19f6549ae944d54c88e3a5f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='tricube', description='kernel'), IntSlider(value=42, description='seed', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nadaraya-Watson kernel weighted average\n",
    "interact_plot_one(\"tricube\",lmbda = fixed(0.4), show_true_y = fixed(True),show_estimated_y=fixed(True),idx_x0=fixed(500),k=fixed(10))\n",
    "# change seed\n",
    "# change polynomial order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the polynomial order\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f_\\theta,x_0) = \\sum_{i=1}^N K_\\lambda(x_0, x_i)\\left(y_i -f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "K nearest neighbours\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\text{Ave}\\left( y_i|x_i\\in N_k(x_0) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Nadarayea-Watson weighted average\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac{\\sum_{i=1}^N K_\\lambda(x_0, x_i)y_i}{\\sum_{i=1}^N K_\\lambda(x_0, x_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Local linear regression\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\beta(x_0)x_i \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\hat\\beta(x_0)x_0.\n",
    "\\end{equation}\n",
    "\n",
    "Local polynomial regression\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta_j(x_0), j=1,\\cdots,d} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\sum_{j=1}^d \\beta_j(x_0)x_i^j \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\sum_{j=1}^N \\hat\\beta(x_0)x_o^j.\n",
    "\\end{equation}\n",
    "\n",
    "------------------------------------------------------------\n",
    "Bias-variance tradeoff\n",
    "\\begin{align}\n",
    "\\text{E}\\hat{f}(x_0) &= \\sum_{i=1}^N l_i(x_0)f(x_i) \\\\\n",
    "&= f(x_0)\\sum_{i=1}^N l_i(x_0) + f'(x_0)\\sum_{i=1}^N (x_i-x_0)l_i(x_0) + \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R,\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\hat{f}(x_0)) = \\sigma^2 \\|l(x_0)\\|^2,\n",
    "\\end{equation}\n",
    "\n",
    "![title](imgs/variance_and_polynomial_order.png) \n",
    "\n",
    "-----------------------------------------------------------------\n",
    "### Local linear regression: matrix formulation and equivalent kernel\n",
    "\n",
    "Define\n",
    "* the vector-valued function $b(x)^T = (1, x)$,\n",
    "* the $N\\times2$ regression matrix $\\mathbf{B}$ with $i$th row $b(x_i)^T$, and\n",
    "* the $N\\times N$ diagonal matrix $\\mathbf{W}(x_0)$ with $i$th diagonal element $K_\\lambda(x_0,x_i)$.\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= b(x_0)^T \\left( \\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B} \\right)^{-1} \\mathbf{B}^T \\mathbf{W}(x_0) \\mathbf{y} \\\\\n",
    "&= \\sum_{i=1}^N l_i(x_0)y_i.\n",
    "\\end{align}\n",
    "\n",
    "Note that $l_i(x_0)$ do not involve $\\mathbf{y}$ and thus the estimate is _linear_ in $y_i$. These weights $l_i(x_0)$ combine the weighting kernel $K_\\lambda(x_0,\\cdot)$ and the least squares operations, and are sometimes referred to as the _equivalent kernel_.\n",
    "\n",
    "FIGURE 6.4 illustrates the effect of local linear regression on the equivalent kernel.\n",
    "\n",
    "![title](imgs/equivalent_kernel.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting kernel\n",
    "\n",
    "### By type of window\n",
    "- **Metric window width**\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D \\left( \\frac{\\|x-x_0\\|}\\lambda \\right)\n",
    "\\end{equation}\n",
    "\n",
    "- **Adaptive window width**\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0, x) = D \\left( \\frac{\\|x-x_0\\|}{h_\\lambda(x_0)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "    - K nearest neighbours \n",
    "\\begin{equation}\n",
    "K_k(x_0, x) = D \\left( \\frac{\\|x-x_0\\|}{\\|x-x_k\\|} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Window width parameter:\n",
    "- the radius of the support region $\\lambda$\n",
    "- standard deviation for Gaussian kernel\n",
    "-  number $k$ of nearest neighbors in $k$-nearest neighborhoods\n",
    "\n",
    "## By functional form\n",
    "- **Compact support**\n",
    "- Epanechnikov\n",
    "\n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac34 (1-t^2) & \\text{if } |t| \\le 1; \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    " - Tri-cube\n",
    "    \n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac{70}{81}(1-|t|^3)^3 & \\text{ if } |t| \\le 1;\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "- Uniform\n",
    "\n",
    "\\begin{equation}\n",
    "D(t) = \\begin{cases}\n",
    "\\frac{1}{2} & \\text{ if } |t| \\le 1;\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- **Infinite support** \n",
    "    - Gaussian \n",
    "\\begin{equation}    \n",
    "K_\\lambda(x_0,x) = \\frac{1}{\\lambda}\\exp\\left(-\\frac{\\|x-x_0\\|^2}{2\\lambda}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "![title](imgs/kernels_textbook.png) \n",
    "\n",
    "\n",
    "### Questions\n",
    "- Can we combine any kernel functional form with any type of window? \n",
    "- Why do we care about the type of window (metric vs. adaptive)? (How does the type of window influence prediction in case of gaps in data, boundaries, sparse areas?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7142f469d4bb4a0faf6e3fca84818fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='knn', description='right'), Text(value='tricube', description='left'), IntSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metric vs. adaptive neighbourhood : illustration at the boundary bias\n",
    "interact_plot_two(right=\"knn\",left=\"tricube\",k_left = fixed(10), k_right = fixed(35),\n",
    "                  lmbda_left = fixed(0.22),lmbda_right = fixed(0.5),\n",
    "                  show_estimated_y = fixed(True),\n",
    "                  show_true_y=fixed(True), pol_order_left = fixed(0),pol_order_right = fixed(0))\n",
    "# slide idx_x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# $\\S$ 6.3. Local Regression in $\\mathbb{R}^p$\n",
    "### Generalizing previous notation\n",
    "\n",
    "Let $b(X)$ be a vector of polynomial terms in $X$ of maximum degree $d$; e.g., with $d=1$ and $p=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2);\n",
    "\\end{equation}\n",
    "\n",
    "with $d=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2);\n",
    "\\end{equation}\n",
    "\n",
    "and trivially with $d=0$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = 1.\n",
    "\\end{equation}\n",
    "\n",
    "At each $x_0 \\in \\mathbb{R}^p$ solve\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - b(x_i)^T\\beta(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "to produce the fit\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = b(x_0)^T \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Typically the kernel will be a radial function, such as the radial Epanechnikov or tri-cube kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|\\cdot\\|$ is the Euclidean norm.\n",
    "\n",
    "Since the Euclidean norm depends on the units in each coordinate, it makes most sense to standardize each predictor, e.g., to unit standard deviation, prior to smoothing.\n",
    "\n",
    "### Problems with high dimension \n",
    "\n",
    "- **Curse of dimensionality**\n",
    "    - **Boundary bias**\n",
    "        - We have seen that there is bias at boundaries, which can be corrected by fitting a polynomial. \n",
    "        - But this comes at a cost of higher variance, which is higher ESPECIALLY at the borders. \n",
    "        - Everything as at border. \n",
    "    - **Points far away from each other**\n",
    "        - impossible to simultaneously maintain localness ($\\Rightarrow$ low bias) and a sizeable sample in the neighborhood ($\\Rightarrow$ low variance) as the dimension increases, without the total sample size increasing exponentially in $p$.\n",
    " \n",
    "- **Vizualization problematic**\n",
    "\n",
    "\n",
    "### Questions: \n",
    "- How to select lambda, polynomial degree? \n",
    "- How to go around the curse of multidimensionality?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.4. Structured Local Regression Models in $\\mathbb{R}^p$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.4.1. Structured Kernels\n",
    "### Standardization\n",
    "\n",
    "One line of approach is to modify the kernel. The default spherical kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right)\n",
    "\\end{equation}\n",
    "\n",
    "gives equal weight to each coordinate, and so a natural default strategy is to standardize each variable to unit standard deviation.\n",
    "\n",
    "A more general approach is to use a positive semidefinite matrix $\\mathbf{A}$ to weigh the different coordinates:\n",
    "\n",
    "\\begin{equation}\n",
    "K_{\\lambda,\\mathbf{A}}(x_0,x) = D \\left( \\frac{(x-x_0)^T\\mathbf{A}(x-x_0)}\\lambda \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Entire coordinates or directions can be downgraded or omitted by imposing appropriate restrictions on $\\mathbf{A}$. For example, if $\\mathbf{A}$ is diagonal, then we can increase or decrease the influence of individual predictors $X_j$ by increasing or decreasing $A_{jj}$.\n",
    "\n",
    "Often the predictors are many and highly correlated, such as those arising from digitized analog signals or images. The covariance function of the predictors can be used to tailor a metric $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.4.2. Structured Regression Functions\n",
    "\n",
    "We are trying to fit a regression function\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{E}(Y|X) = f(X_1,X_2,\\cdots,X_p)\n",
    "\\end{equation}\n",
    "\n",
    "in $\\mathbb{R}^p$, in which every level of interaction is potentially present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure via ANOVA decomposition\n",
    "It is natural to consider ANOVA decompositions of the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(X_1,X_2,\\cdots,X_p) = \\alpha + \\sum_j g_j(X_j) + \\sum_{k<l} g_{kl}(X_k,X_l) + \\cdots\n",
    "\\end{equation}\n",
    "\n",
    "and then introduce structure by eliminating some of the higher-order terms.\n",
    "\n",
    "Additive models assume only main effect terms:\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha + \\sum_{j=1}^p g_j(X_j);\n",
    "\\end{equation}\n",
    "\n",
    "Second-order models will have terms with interactions of order at most two, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfitting\n",
    "In Chapter 9, we describe iterative _backfitting_ algorithms for fitting such lower-order interaction models. In the additive model, for example, if all but the $k$th term is assumed known, then we can estimate $g_k$ by local regression of $Y - \\sum_{j\\neq k}g_j(X_j)$ on $X_k$. This is done for each function in turn, repeatedly, until convergence. The important detail is that at any stage, 1D local regression is all that is needed. The same ideas can be used to fit low-dimensional ANOVA decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying coefficients models\n",
    "An important special case of these structured models are the class ofr _varying coefficient models_. Suppose that we divide the $p$ predictors in $X$ into a set $(X_1, X_2, \\cdots, X_q)$ with $q<p$, and the remainder of the variables we collect in the vector $Z$. We then assume the conditionally linear model\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha(Z) + \\beta_1(Z)X_1 + \\cdots + \\beta_q(Z)X_q.\n",
    "\\end{equation}\n",
    "\n",
    "For given $Z$, this is a linear model, but each of the coefficients can vary with $Z$. It is natural to fit such a model by locally weighted least squares:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(z_0),\\beta(z_0)} \\sum_{i=1}^N K_\\lambda(z_0,z_i) \\left( y_i - \\alpha(z_0) - x_{i1}\\beta_1(z_0) - \\cdots - x_{qi}\\beta_q(z_0) \\right)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.5. Local Likelihood and Other Models\n",
    "\n",
    "The concept of local regression and varying coefficient models is extremely broad: Any parametric model can be made local if the fitting method accommodates observation weights. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with each observation $y_i$ is a parameter\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_i = \\theta(x_i) = x_i^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "linear in the covariate(s) $x_i$, and inference for $\\beta$ is based on the log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta) = \\sum_{i=1}^N l(y_i,x_i^T\\beta).\n",
    "\\end{equation}\n",
    "\n",
    "We can model $\\theta(X)$ more flexibly by using the likelihood local to $x_0$ for inference of $\\theta(x_0) = x_0^T\\beta(x_0)$:\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta(x_0)) = \\sum_{i=1}^N K_\\lambda(x_0,x_i) l(y_i,x_i^T\\beta(x_0)).\n",
    "\\end{equation}\n",
    "\n",
    "Many likelihood models, in particular the family of  generalized linear models including logistic and log-linear models, involve the covariates in a linear fashion. Local likelihood allows a relaxation from a globally linear model to one that is locally linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.6. Kernel Density Estimation and Classification\n",
    "\n",
    "## Kernel density estimation\n",
    "* unsupervised learning procedure, which historically precedes kernel regression\n",
    "* leads naturally to a simple family of procedures for nonparametric classification\n",
    "* suppose we have a random sample $x_1,\\cdots,x_N$ drawn from a probability density $f_X(x)$, $X\\in\\mathbb{R}$ for simplicity, and we wish to estimate $f_X$ at a point $x_0$\n",
    "\n",
    "A) **natural local estimate** (bumpy):\n",
    "\\begin{equation}\n",
    "\\hat{f}_X(x_0) = \\frac{\\#x_i \\in \\mathcal{N}(x_0)}{N\\lambda},\n",
    "\\end{equation}\n",
    "\n",
    "   * $\\mathcal{N}(x_0)$ is a small metric neighborhood around $x_0$ of width $\\lambda$\n",
    "   \n",
    "B) ***Parzen* estimate** (smooth):\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N\\lambda} \\sum_{i=1}^N K_\\lambda(x_0,x_i),\n",
    "\\end{equation}\n",
    "* popular choice for $K_\\lambda$ is the Gaussian kernel:\n",
    "\\begin{align}\n",
    "K_\\lambda(x_0,x) = \\phi\\left(\\frac{|x-x_0|}\\lambda\\right) \\\\\n",
    "\\hat{f}_X(x)= \\frac1N \\sum_{i=1}^N \\phi_\\lambda(x-x_i)= \\left( \\hat{F}\\star\\phi_\\lambda \\right)(x)\\\\\n",
    "\\end{align}\n",
    "\n",
    "  * $\\star$ denotes convolution\n",
    "  * $\\phi_\\lambda$ denote the Gaussian density with mean zero and standard-deviation $\\lambda$\n",
    "  * $\\hat{F}$ is the sample empirical distribution - puts mass $1/N$ at each observation \n",
    "  * $\\hat{F}$ smoothed by adding independent Gaussian noise to each observation\n",
    "\n",
    "* Parzen density estimate is the equivalent of local average, improvements have been proposed along the lines of local regression\n",
    "* natural generalization of the Gaussian density estimate for $\\mathbb{R}^p$ amounts to using the Gaussian product kernel\n",
    "* example: estimation of density of blood pressure in CHD group, at each point averages all the kernels at that point, these are scaled down by a factor of 10 to make it readable:\n",
    "\n",
    "![title](imgs/6_6_a_blood_pressure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Classification\n",
    "\n",
    "* one can use nonparametric density estimates for classification in a straight-forward fashion using Bayes' theorem\n",
    "\n",
    "* assume $J$ class problem\n",
    "* fit nonparametric density estimates $\\hat{f}_j(X)$, for $j=1,\\cdots,J$ separately in each of the classes\n",
    "* get estimates of the class priors $\\hat\\pi_j$ (usually the sample proportions)\n",
    "* then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\text{Pr}}(G=j|X=x_0) = \\frac{\\hat\\pi_j \\hat{f}_j(x_0)}{\\sum_{k=1}^J \\hat\\pi_k \\hat{f}_k(x_0)}.\n",
    "\\end{equation}\n",
    "\n",
    "* example: estimation of density of blood pressure in CHD vs non-CHD groups using Gaussian kernel density estimate in each, right panel shows the estimated posterior probabilities for CHD:\n",
    "\n",
    "![title](imgs/6_6_b_blood_pressure.png)\n",
    "![title](imgs/6_6_b_blood_pressure_compare.png)\n",
    "\n",
    "* for high SBP the data are sparse in both classes, density estimates low and of poor quality \n",
    "* local logistic regression method uses the tri-cube kernel with k-NN bandwidth; this eﬀectively widens the kernel in this region, and makes use of the local linear assumption to smooth out the estimate\n",
    "\n",
    "* learning the separate densities from data: possibly rougher, high-variance ﬁt to capture specific features\n",
    "* if classiﬁcation is the ultimate goal, we need only to estimate the posterior well near the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Naive Bayes Classifier**\n",
    "\n",
    "* espeically appropriate when the dimension $p$ of the feature space is high\n",
    "* **Naive Bayes Model** assumes that given a class $G=j$, the features $X_k$ are independent:\n",
    "\n",
    "\\begin{equation}\n",
    "f_j(X) = \\prod_{k=1}^p f_{jk}(X_k)\n",
    "\\end{equation}\n",
    "\n",
    "* in traditional approach Gaussian density functions\n",
    "* generally not true, but simplifies the estimation dramatically\n",
    "* class-conditional marginal densities $f_{jk}$ can each be estimated separately\n",
    "* if a component $X_j$ of $X$ is discrete, use an appropriate histogram\n",
    "* often outperforms far more sophisticated alternatives: bias of marginal densities may not be a big problem and the naive approach saves a lot of variance\n",
    "\n",
    "* logit-transform (using class J as the base):\n",
    "\\begin{align}\n",
    "\\log\\frac{\\text{Pr}(G=l|X)}{\\text{Pr}(G=J|X)} &= \\log\\frac{\\pi_l f_l(X)}{\\pi_J f_J(X)}\\\\\n",
    "&= \\log\\frac{\\pi_l \\prod_{k=1}^p f_{lk}(X_k)}{\\pi_J \\prod_{k=1}^p f_{Jk}(X_k)}\\\\\n",
    "&= \\log\\frac{\\pi_l}{\\pi_J} + \\sum_{k=1}^p \\log\\frac{f_{lk}(X_k)}{f_{Jk}(X_k)}\\\\\n",
    "&= \\alpha_l + \\sum_{k=1}^p g_{lk}(X_k).\n",
    "\\end{align}\n",
    "\n",
    "* form of a *generalized additive model* (Chapter 9), but is fit in different ways \n",
    "* relationship between naive Bayes and generalized additive models is analogous to that between linear discriminant analysis and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.7. Radial Basis Functions and Kernels\n",
    "\n",
    "* functions can be represented as expansions in **basis functions** (Chapter 5):\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^M \\beta_j h_j(x).\n",
    "\\end{equation}\n",
    "\n",
    "* flexibility of the modeling depends on:\n",
    "  * picking an appropriate family of basis functions\n",
    "  * controlling the complexity of the representation by selection, regularization, or both\n",
    "\n",
    "* some of the families of basis functions have elements that are defined locally (e.g. B-splines)\n",
    "* if more flexibility desired in a particular region, then that region needs to be represented by more basis functions (more knots in the case of B-splines)\n",
    "* tensor products of $\\mathbb{R}$-local basis functions deliver basis functions local in $\\mathbb{R}^p$\n",
    "* even basis functions which are not local (e.g. truncated power bases for splines) can compose function $f(x)$ with local behavior - global effects are cancelled out\n",
    "\n",
    "* **kernel methods** achieve flexibility by fitting simple models in a region local to the target point $x_0$.\n",
    "* **radial basis functions** combine the kernel methods and the representation using basis functions by treating the kernel functions $K_\\lambda(\\xi,x)$ as basis functions\n",
    "\\begin{align}\n",
    "f(x) &= \\sum_{j=1}^M K_{\\lambda_i}(\\xi_j,x)\\beta_j \\\\\n",
    "&= \\sum_{j=1}^M D\\left( \\frac{\\|x-\\xi_j\\|}{\\lambda_j} \\right)\\beta_j\n",
    "\\end{align}\n",
    "\n",
    "  * each basis element is indexed by a location or _prototype_ parameter $\\xi_j$ and a scale parameter $\\lambda_j$\n",
    "  * popular choice for $D$ is the standard Gaussian density function\n",
    "  * several approaches to find the parameters $\\{\\lambda_j, \\xi_j, \\beta_j\\}$, for $j=1,\\cdots,M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "* the textbook uses Gaussian kernel and least squares methods\n",
    "\n",
    "A) Optimize the sum-of-squares with respect to all the parameters\n",
    "\\begin{equation}\n",
    "\\min_{\\{\\lambda_j,\\xi_j,\\beta_j\\}_1^M} \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^M \\beta_j \\exp\\left( -\\frac{(x_i-\\xi_j)^T(x_i-\\xi_j)}{\\lambda_j^2} \\right)\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "* the model is commonly referred to as an RBF network, an alternative to the sigmoidal neural network; the $\\xi_j$ and $\\lambda_j$ playing the role of the weights\n",
    "* the criterion is nonconvex with multiple local minima, and the algorithms for optimization are similar to those used for neural networks.\n",
    "\n",
    "B) Estimate the $\\{\\lambda_j,\\xi_j\\}$ separately from the $\\beta_j$\n",
    "* get $\\{\\lambda_j,\\xi_j\\}$, often chosen in an unsupervised way using the $X$ distribution alone\n",
    "  * e.g. fit a Gaussian mixture density model to the training $x_i$, which provides both the centers $\\xi_j$ and the scales $\\lambda_j$.\n",
    "  * other option mentioned: clustering methods\n",
    "* given $\\{\\lambda_j,\\xi_j\\}$, the estimation of $\\beta_j$ is a simple least squares problem \n",
    "\n",
    "C) Assume constant $\\lambda_j=\\lambda$\n",
    "* this creates *holes*-regions of $\\mathbb{R}^p$ where none of the kernels has appreciable support\n",
    "\n",
    "![title](imgs/6_7_holes_regions.png)\n",
    "\n",
    "* renormalization of the radial basis functions helps\n",
    "\n",
    "\\begin{equation}\n",
    "h_j(x) = \\frac{D(\\|x-\\xi_j\\|/\\lambda)}{\\sum_{k=1}^M D(\\|x-\\xi_k\\|/\\lambda)}\n",
    "\\end{equation}\n",
    "\n",
    "* e.g. Nadaraya-Watson kernel regression estimator in $\\mathbb{R}^p$\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= \\sum_{i=1}^N y_i \\frac{K_\\lambda(x_0,x_i)}{\\sum_{j=1}^N K_\\lambda(x_0,x_j)} \\\\\n",
    "&= \\sum_{i=1}^N y_i h_i(x_0),\n",
    "\\end{align}\n",
    "\n",
    "* with a basis function $h_i$ located at every observation and coefficients $y_i$:\n",
    "$\\xi_i=x_i$, $\\hat\\beta_i=y_i$, for $i=1,\\cdots,N$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.8. Mixture Models for Density Estimation and Classification\n",
    "\n",
    "* **mixture model** is a useful tool for density estimation, and can be viewed as a kind of kernel method\n",
    "* Gaussian mixture model:\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{m=1}^M \\alpha_m\\phi(x;\\mu_m,\\mathbf{\\Sigma}_m)\n",
    "\\end{equation}\n",
    " \n",
    "  * mixing proportions $\\alpha_m$, $\\sum\\alpha_m=1$\n",
    "  * each Gaussian density has a mean $µ_m$ and covariance matrix $Σ_m$\n",
    "\n",
    "\n",
    "* fit usually by maximum likelihood, using the EM algorithm (Chapter 8)\n",
    "* probability that $x_i$ belongs to component $m$:\n",
    "\\begin{equation}\n",
    "\\hat{r}_{im} = \\frac{\\hat{\\alpha}_m\\phi(x_i;\\hat{\\mu}_m,\\hat{\\mathbf{\\Sigma}}_m)}{\\sum_{k=1}^M\\hat{\\alpha}_k\\phi(x_i;\\hat{\\mu}_k,\\hat{\\mathbf{\\Sigma}}_k)}\n",
    "\\end{equation}\n",
    "* on CHD data predicts comparably to linear logistic regression fit by MLE (error rate 32%)\n",
    "\n",
    "| Mixture model |  No | Yes |\n",
    "| ------------- | --- | --  |\n",
    "| **CHD No**    | 232 |  70 |\n",
    "| **CHD Yes**   | 76  |  84 |\n",
    "\n",
    "### Special cases\n",
    "\n",
    "* if covariance matrices constrained to be scalar: $\\mathbf{\\Sigma}_m=\\sigma_m\\mathbf{I}$, then the mixture model has the form of a radial basis expansion\n",
    "* if in addition $\\sigma_m = \\sigma >0$ and $M\\uparrow N$, then the maximum likelihood estimate approaches the kernel density estimate \n",
    "\n",
    "### Advanced applications\n",
    "* Using Bayes' theorem, separate mixture densities in each class lead to flexible models for $\\text{Pr}(G|X)$ (Chapter 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart disease example\n",
    "\n",
    "![title](imgs/CHD_Mixture.png) \n",
    "\n",
    "* histograms of Age for the no CHD and CHD groups separately, and combined\n",
    "* estimated densities from a Gaussian mixture model\n",
    "  * estimated component densities (blue and orange) \n",
    "  * estimated mixture density (green)\n",
    "* orange density has a very large standard deviation, and approximates a uniform density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.9. Computational Considerations\n",
    "\n",
    "* kernel, local regression and density estimation are **memory-based methods**: the model is the entire training data set, and the fitting is done at evaluation or prediction time\n",
    "* the computational cost to fit at a single obervation $x_0$ is $O(N)$ flops, except in oversimplified cases\n",
    "* the smoothing parameter(s) $\\lambda$ for kernel methods are typically determined off-line, at a cost of $O(N^2)$ flops\n",
    "* by comparison, an expansion in $M$ basis functions costs $O(M)$ for one evaluation, and typically $M\\sim O(\\log N)$\n",
    "* basis function methods have an initial cost of at least $O(NM^2+M^3)$\n",
    "*  triangulation schemes can reduce the computations (e.g. $\\textsf{loess}$ in S-Plus or R): compute the fit exactly at $M$ locations ($O(NM)$), and then use blending techniques to interpolate the fit elsewhere ($O(M)$ per evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of local likelihood, we consider the local version of the multiclass linear logistic regression model of Chapter 4. The data consist of features $x_i$ and an associated categorical response $g_i \\in \\{1,2,\\cdots,J\\}$, and the linear model has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(G=j|X=x) = \\frac{\\exp\\left(\\beta_{j0} + \\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\beta_{k0} + \\beta_k^Tx\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "The local log-likelihood for this $J$ class model can be written\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left\\{ \\beta_{g_i 0}(x_0) + \\beta_{g_i}^T(x_i-x_0) - \\log\\left( 1 + \\sum_{k=1}^{J-1} \\exp\\left( \\beta_{k0}(x_0) + \\beta_k(x_0)^T(x_i-x_0) \\right) \\right) \\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Notice that\n",
    "* we have used $g_i$ as a subscript in the first line to pick out the appropriate numerator;\n",
    "* $\\beta_{J0} = 0$ and $\\beta_J = 0$ by the definition of the model;\n",
    "* we have centered the local regressions at $x_0$, so that the fitted posterior probabilities at $x_0$ are simply  \n",
    "\n",
    "  \\begin{equation}\n",
    "  \\hat{\\text{Pr}}(G=j|X=x) = \\frac{\\exp\\left(\\hat\\beta_{j0} + \\hat\\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\hat\\beta_{k0} + \\hat\\beta_k^Tx\\right)}.\n",
    "  \\end{equation}\n",
    "  \n",
    "This model can be used for flexible multiclass classification in moderately low dimensions, although successes have been reported with the high-dimensional ZIP-code classficiation problem. Generalized additive models (Chapter 9) using kernel smoothing methods are closely related, and avoid dimensionality problems by assuming an additive structure for the regression function."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
