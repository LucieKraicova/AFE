{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Advanced Financial Econometrics\n",
    "## PhD seminar reading group\n",
    "\n",
    "#### Winter Semester 2019/2020\n",
    "\n",
    "#### Week 4 (Nove 6 2019): Chapter 4\n",
    "\n",
    "by Lukas, Marek and Nicolas\n",
    "\n",
    "in Python\n",
    "\n",
    "---\n",
    "\n",
    "Have a look here: 10 important papers to get started with machine learning: https://medium.com/@matthiasbitzer94/10-important-papers-to-get-started-with-machine-learning-66fc88a39e71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nicolas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Intro: Deep Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of **deep feedforward networks**, also called **feedforward neural networks**, or **multilayer perceptrons(MLPs)**, is to approximate some function $f$. \n",
    "\n",
    "A feedforward network deﬁnes a mapping $\\mathbf{y} = f(\\mathbf{x}; \\mathbf{\\theta})$ and learns the value of the parameters θ that resultin the best function approximation.\n",
    "\n",
    "Feedforward form the basis of many important commercial applications. **Convolutional networks** used for object recognition from photos are aspecialized kind of feedforward network. Feedforward networks are a conceptualstepping stone on the path to **recurrent networks**, which power many naturallanguage applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $$\\mathbb{E}_{x\\sim P} \\left[ f(x)\\right] = \\int{p(x)f(x)}dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little bit of History: Is Deep Learning cool new stuff?\n",
    "\n",
    "Dates back to 1940, has been rebranded many times\n",
    "\n",
    "Three waves:\n",
    "\n",
    "* deep learning known as cybernetics in the 1940s–1960s\n",
    "* deep learning known as connectionism in the 1980s–1990s\n",
    "* the current resurgence under the name deep learning beginning in 2006.\n",
    "\n",
    "Since earliest algorithm were intended as models of biological learning (models of what could happen in the brain), deep learning got name **artificial neural networks (ANNs)**.\n",
    "\n",
    "**Modern deep learning** goes beyond this perspective and is more general: *multiple levels of compositions*\n",
    "\n",
    "Predecessors of modern deep learning are **simple linear regressions** taking inputs $x_1,\\ldots,x_n$ and associate them to $y$ with set of weights $w_1,\\ldots,w_n$ in a linear fashion.\n",
    "\n",
    "#### Why Deep Learning only now?\n",
    "\n",
    "We are simply able to provide muchh better the algorithm with what they need = **increasing dataset sizes**\n",
    "\n",
    "challenge: how to improve performance on smaller and unlabelled sets (currently supervised deep learning matches human performance when trained on 10 million labaled examples).\n",
    "\n",
    "**increasing model sizes, accuracy, complexity**\n",
    "\n",
    "#### 2. ML Basics\" Linear Algebra (Ch 2) and \n",
    "\n",
    "* **Scalars** - single number $s \\in \\mathbb{R}$\n",
    "* **Vectors** - array of numbers (ordered), vector of $n$ elements lies in the set formed by taking Cartesian produc of $\\mathbb{R}$ $n$ times denoted as $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = rand(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Matrices** - 2D array of numbers $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [(i+j)%2 for i=1:8, j=1:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tensors** - array of numbers arranged on a regular grid with a variable number of axes $\\mathsf{A}$ with elements $A_{i,j,k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = rand(0:10, (3,4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... revision\n",
    "\n",
    "#### 3. Probability and Information Theory\n",
    "\n",
    "Probability theory is a mathematical framework for representing uncertain statements.\n",
    "\n",
    "It provides a **means of quantifying uncertainty** and **axioms for deriving new uncertain statements**. \n",
    "\n",
    "\n",
    "\n",
    "**Expectation**\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim P} \\left[ f(x)\\right] = \\sum_x{P(x)f(x)}$$\n",
    "\n",
    "or with continuous variable\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim P} \\left[ f(x)\\right] = \\int{p(x)f(x)}dx$$\n",
    "\n",
    "\n",
    "Information theory allows us to tell something about probability distributions, or uncertainty\n",
    "\n",
    "**Information theory can be summarized by**\n",
    "\n",
    "* **Information**: $$I(x) = -\\log P(x)$$\n",
    "\n",
    "* **Entropy**: $$H(\\mathsf{x}) = \\mathbb{E}_{x\\sim P} \\left[ I(x)\\right]$$\n",
    "\n",
    "Shannon entropy of a distribution is the expected amount of information in event drawn from that distribution\n",
    "\n",
    "*Example*: Coin with probability of heads = 1 and tails = 0 has $H(x)=0$ (no uncertainty) and coin with equal probabilities has $H(x)=1$ (uncertain)\n",
    "\n",
    "\n",
    "* **Kullback-Leibler (KL) divergence** $$D_{KL}(P||Q) = \\mathbb{E}_{x\\sim P} \\left[ \\log \\frac{P(x)}{Q(x)}\\right] =\\mathbb{E}_{x\\sim P} \\left[ \\log P(x) - \\log Q(x)\\right]$$\n",
    "\n",
    "measure of distance between two distributions (in neural networks, it is a loss function comparing distribution of data and outcome of network)\n",
    "\n",
    "#### 4. Numeric computation (Chapter 4)\n",
    "\n",
    "Short, but many important things: optimization, Jacobian and Hessian\n",
    "\n",
    "**Overflow and Underflow**\n",
    "\n",
    "Continuous math is difficult to represent on digital computer since we need infinitely many real numbers with finite number of bit patterns ==> approximation errors\n",
    "\n",
    "numerical error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2.0^(-52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2.0^-53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[log(2.0^(-1022-52));\n",
    " log(2.0^(-1022-53))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devastating rounding error may be \n",
    "* **underflow** when number near zero is rounded to zero (try to divide with zero, or use log(0))\n",
    "* **overflow** when numbers with large magnitude are approximated as $\\infty$ of $-\\infty$ resulting soon in NANs\n",
    "\n",
    "Solution could be softmax function\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^n \\exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(x) = exp.(x)./sum(exp.(x))\n",
    "\n",
    "c=typemax(Int)-1\n",
    "softmax(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Because exp overflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution is softmax(z) where $z=x+a$ since sofmax(z) = sofmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimisation** \n",
    "\n",
    "Everything is about optimisation problem\n",
    "\n",
    "$$\\arg \\min_{\\theta} \\mathcal{L}(\\theta)$$\n",
    "\n",
    "where $\\mathcal{L}(\\theta)$ is loss. In linear regression it is simple, but $f(\\theta,x)\\rightarrow y$ can become tricky soon.\n",
    "\n",
    "*so it is good to know how it works*\n",
    "\n",
    "btw why do not we maximize? (AE no1. warmup question)\n",
    "\n",
    "So we minimize **objective function** or **criterion**, **cost function**, **loss function**, or **error function**\n",
    "\n",
    "**Gradient-Based Optimization**\n",
    "\n",
    "The **derivative** of a function $f$, denoted as $f'(x)$, specifies how a small change in input reflects as a change in output: $f(x + \\epsilon) \\approx f(x) + \\epsilon * f'(x)$. \n",
    "\n",
    "It is useful for minimizing function since it tells us **how to change $x$ in order to make small improvement in $y$**\n",
    "\n",
    "This technique is called **gradient descent** (Cauchy, 1847).\n",
    "\n",
    "![gradient descent](images/gradient_descent.png)\n",
    "\n",
    "$f'(x)=0$ provide no information \n",
    "\n",
    "are called **critical** or **stationary** points. Types of critical points:\n",
    "\n",
    "![critical points](images/critical_points.png)\n",
    "\n",
    "A **local minimum** is point where $f(x)$ is lower than at all neighboring points, so it is no longer possible to decrease $f(x)$ by making infinitesimal steps.\n",
    "\n",
    "How to get **global minimum**?\n",
    "\n",
    "For functions with multiple inputs, **partial derivative** $\\frac{\\delta}{\\delta x_i}f(x)$ measures how $f$ changes as only the variable $x_i$ changes at point $x$. The **gradient** of $f$ is a vector containing all partial derivatives denoted $\\nabla_x\\, f(x)$. The **directional derivative** in a direction ***u*** (unit vector) is the slope of $f$ in the direction *u*.\n",
    "\n",
    "i.e. the directional derivative is the value of $\\frac{\\delta}{\\delta \\alpha}f(x+\\alpha*u)$ evaluated as $\\alpha \\rightarrow 0$.\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\\frac{\\delta}{\\delta \\alpha}f(x+\\alpha*u) = \\big(\\frac{\\delta}{\\delta \\alpha}(x+\\alpha*u)\\big)^T\\frac{\\delta}{\\delta(x+\\alpha*u)}f(x+\\alpha*u)$\n",
    "\n",
    "as $\\alpha$ tends to 0 the expression reduces to $u^T\\nabla_x\\, f(x)$. To minimize $f$ we need to find the direction *u* in which $f$ decreases the fastest i.e.:\n",
    "\n",
    "![partial derivative](images/partial_derivative.png)\n",
    "\n",
    "Ignoring terms not relating to *u* we see that function *f* is decreased most when $cos\\theta = -1$ i.e. we move in the direction opposite to the gradient and $\\theta$ is angle. \n",
    "\n",
    "We can decrease $f$ by moving in the directino of negative gradient. This is the method of **steepest descent** or **gradient descent**. \n",
    "\n",
    "Steepest descent proposes the new point: \n",
    "$$x' = x - \\epsilon \\nabla_x\\, f(x)$$ \n",
    "\n",
    "where $\\epsilon$ is the **learning rate**. $\\epsilon$ is positive scalar determining size of step or can be solved analytically to make the gradient vanish. Another approach is to try different values of $\\epsilon$ and choose the value that causes the most decrease (**line search**).\n",
    "\n",
    "The general concept of repeatedly making a small move in the locally best direction can be generalized to discrete spaces (**hill climbing**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x)\n",
    "    (x-2)^2\n",
    "end\n",
    "\n",
    "function df01(x)\n",
    "    2*(x-2)\n",
    "end\n",
    "\n",
    "xs = Float64[]\n",
    "xy = Float64[]\n",
    "x0=4;\n",
    "\n",
    "xs=zeros(10)\n",
    "\n",
    "for iter in 1:10\n",
    "       xs[iter] = x0\n",
    "       ys[iter] = f(x0)\n",
    "       x0   =  x0 - 0.9 * df01(x0)\n",
    "end\n",
    "\n",
    "i=0\n",
    "k=zeros(31)\n",
    "l=zeros(31)\n",
    "for a in -1:0.2:5\n",
    "    i=i+1\n",
    "    k[i]=a\n",
    "    l[i]=(a-2).^2\n",
    "end\n",
    "\n",
    "plot(\n",
    "layer(x=k, y=l, Geom.line()),\n",
    "layer(x=xs,y=ys,Geom.point()),\n",
    "layer(x=xs[1:2],y=ys[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[2:3],y=ys[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[3:4],y=ys[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[4:5],y=ys[4:5],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[5:6],y=ys[5:6],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[6:7],y=ys[6:7],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[7:8],y=ys[7:8],Geom.line(),Theme(default_color=colorant\"orange\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can easily diverge, for example change learning rate to 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x)\n",
    "    (x-2)^2\n",
    "end\n",
    "\n",
    "function df01(x)\n",
    "    2*(x-2)\n",
    "end\n",
    "\n",
    "xs = Float64[]\n",
    "xy = Float64[]\n",
    "x0=4;\n",
    "\n",
    "xs=zeros(10)\n",
    "\n",
    "for iter in 1:10\n",
    "       xs[iter] = x0\n",
    "       ys[iter] = f(x0)\n",
    "       x0   =  x0 - 1.05 * df01(x0)\n",
    "end\n",
    "\n",
    "i=0\n",
    "k=zeros(31)\n",
    "l=zeros(31)\n",
    "for a in -5:0.5:10\n",
    "    i=i+1\n",
    "    k[i]=a\n",
    "    l[i]=(a-2).^2\n",
    "end\n",
    "\n",
    "plot(\n",
    "layer(x=k, y=l, Geom.line()),\n",
    "layer(x=xs,y=ys,Geom.point()),\n",
    "layer(x=xs[1:2],y=ys[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[2:3],y=ys[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[3:4],y=ys[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[4:5],y=ys[4:5],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[5:6],y=ys[5:6],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[6:7],y=ys[6:7],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=xs[7:8],y=ys[7:8],Geom.line(),Theme(default_color=colorant\"orange\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLS with gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function df(x, y, b)\n",
    "sum(- (y - x*b)' * x)\n",
    "end\n",
    "\n",
    "rnorm = Normal()\n",
    "x = rand(rnorm, 500)\n",
    "y = x * 2 + rand(rnorm, 500)\n",
    "b    = 0.0\n",
    "bold = 1.0\n",
    "bvals = Float64[]\n",
    "\n",
    "while (sum(abs(b - bold)) > 1e-10)\n",
    "push!(bvals, b)\n",
    "       bold = b\n",
    "       der = df(x, y, bold)\n",
    "       b   =  b - 0.003 * der\n",
    "end\n",
    "\n",
    "plot(Guide.title(\"True beta: 2, est. beta: \" * string(b)),\n",
    "layer(x=x, y=y, Geom.point(), Theme(default_color=colorant\"orange\")),\n",
    "layer(x=x,y=x*bvals[3], Geom.line(), Theme(default_color=colorant\"red\")),\n",
    "layer(x=x,y=x*bvals[5], Geom.line(), Theme(default_color=colorant\"black\")),\n",
    "layer(x=x,y=x*bvals[8], Geom.line(), Theme(default_color=colorant\"green\")),\n",
    "layer(x=x,y=x*b, Geom.line()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(x'x)x'y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "k=zeros(21)\n",
    "l=zeros(21)\n",
    "for a in 1:0.1:3\n",
    "    i=i+1\n",
    "    k[i]=a\n",
    "    l[i]=sum((y - x*a).^2)\n",
    "end\n",
    "\n",
    "i=0\n",
    "kk=zeros(10)\n",
    "lgd=zeros(10)\n",
    "for c in 2:11\n",
    "    i=i+1\n",
    "    kk[i]=bvals[c]\n",
    "    lgd[i]=sum((y - x*bvals[c]).^2)\n",
    "end\n",
    "plot(\n",
    "layer(x=k, y=l, Geom.line()),\n",
    "layer(x=kk,y=lgd,Geom.point()),\n",
    "layer(x=kk[1:2],y=lgd[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=kk[2:3],y=lgd[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=kk[3:4],y=lgd[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=kk[4:5],y=lgd[4:5],Geom.line(),Theme(default_color=colorant\"orange\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beyond gradient: Jacobian and Hessian Matrices**\n",
    "\n",
    "If we have a function $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, then the **Jacobian** matrix $J \\in \\mathbb{R}^{n\\, \\times\\, m}$ of $f$ is defined such that $J_{i,j} = \\frac{\\delta}{\\delta x_j}f_i(x)$ and holds all partial derivatives of a function.\n",
    "\n",
    "The **second derivative** tells us how the first derivative changes with small changes in input. It is a measurement of **curvature** andis important since it tells whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone.\n",
    "\n",
    "* $f''(x)<0 \\rightarrow$ function curves **downwards**\n",
    "* $f''(x)>0 \\rightarrow$ function curves **upwards**\n",
    "\n",
    "<img src=\"images/fig5.png\" width=\"50%\">\n",
    "\n",
    "The **Hessian** matrix **H**(f)(**x**) is defined such that\n",
    "\n",
    "$H(f)(x)_{i,j} = \\frac{\\delta^2}{\\delta x_i \\delta x_j}f(x)$\n",
    "\n",
    "Hessian is the Jacobian of the gradient.\n",
    "\n",
    "Anywhere that the second partial derivatives are continuous, the differential operators are commutative. This means that $H_{i,j} = H_{j,i}$ and the Hessian is symmetric. This is the most common case in the deep learning regime.\n",
    "\n",
    "Hessian matrix is real and symmetric $\\Rightarrow$ it can be decomposed into a set of real eigenvalues and orthogonal eigenvector basis.\n",
    "\n",
    "Second derivative in a specific direction **d**(unit vector) is $\\mathbf{d^THd}$.\n",
    "\n",
    "When **d** is an eigenvector of H, the second derivative is the corresponding eigenvalue. In the general case, the second derivative is given by the weighted average of eigenvalues. The minimum eigenvalue determines the minimum second derivative.\n",
    "\n",
    "The (directional) second derivative tells us how well we can expect a gradient descent step to perform. Second-order Taylor series approximation of $f(\\mathbf{x})$ around the point $\\mathbf{x^{(0)}}$:\n",
    "\n",
    "$f(\\mathbf{x}) \\approx f(\\mathbf{x^{(0)}}) + (\\mathbf{x}-\\mathbf{x^{(0)}})^T\\mathbf{g} + \\frac{1}{2}(\\mathbf{x}-\\mathbf{x^{(0)}})^T\\mathbf{H}(\\mathbf{x}-\\mathbf{x^{(0)}})$ where **g** is the gradient and **H** is the Hessian\n",
    "\n",
    "Using gradient descent, and learning rate $\\epsilon$ the new point will be $(\\mathbf{x^{(0)}} - \\epsilon \\mathbf{g})$. Substituting in the above equation:\n",
    "\n",
    "$f(\\mathbf{x^{(0)}} - \\epsilon \\mathbf{g}) \\approx f(\\mathbf{x^{(0)}}) - \\epsilon \\mathbf{g}^T\\mathbf{g} + \\frac{1}{2}\\epsilon^2\\mathbf{g}^T\\mathbf{H}\\mathbf{g}$\n",
    "\n",
    "* Orignal function\n",
    "* expected decrease due to gradient\n",
    "* correction due to function curvature. \n",
    "\n",
    "When the last term is large, the update actually moves the point uphill. When it is zero or negative, the equation gives that larger $\\epsilon$ will always decrease the function value, however, moving too far from $\\mathbf{x^{(0)}}$ will invalidate the Taylor approximation. \n",
    "\n",
    "At critical point, where $f'(x) = 0$, the second derivative can be used to determine local maximum or minimum. This is **second derivative test**\n",
    "\n",
    "| $f''(x)\\, $ | conclusion |\n",
    "| --- | --- |\n",
    "|  $>0$ | local minimum |\n",
    "|  $<0$ | local maximum |\n",
    "|  $=0$ | inconclusive |\n",
    "\n",
    "In multiple dimensions, for Hessian matrix:\n",
    "\n",
    "| eigenvalue | conclusion |\n",
    "| --- | --- |\n",
    "| all positive | local minimum |\n",
    "| all negative | local maximum |\n",
    "| atleast one positive and negative each | saddle |\n",
    "| all non-zero same sign, atleast one zero | inconclusive |\n",
    "\n",
    "<img src=\"images/saddle.png\" width=\"30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M<ultiple dimensions: 2D optimisation**\n",
    "\n",
    "Sems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x, y)\n",
    "    2*x^2+y^2\n",
    "end\n",
    "\n",
    "function df01(x)\n",
    "    4*x\n",
    "end\n",
    "\n",
    "function df02(y)\n",
    "    2*y\n",
    "end\n",
    "\n",
    "xs = Float64[]\n",
    "x0=1\n",
    "y0=1\n",
    "\n",
    "xs=zeros(10)\n",
    "ys=zeros(10)\n",
    "\n",
    "for iter in 1:10\n",
    "       xs[iter] = x0;\n",
    "       ys[iter] = y0;\n",
    "       x0   =  x0 - 0.4 * df01(x0)\n",
    "       y0   =  y0 - 0.4 * df02(y0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    layer(z=(x,y) -> 2*x^2+y^2, x=linspace(-2,2,30), y=linspace(-2,2,30), Geom.contour),\n",
    "    layer(x=xs,y=ys,Geom.point()),\n",
    "    layer(x=xs[1:2],y=ys[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[2:3],y=ys[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[3:4],y=ys[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[4:5],y=ys[4:5],Geom.line(),Theme(default_color=colorant\"orange\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in more dimensions there is different second derivative for each directin at a single point. The condition number tells how much second derivatives differ from each other.\n",
    "\n",
    "bad conditioning means that they do not differ much and gradient descent works poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x, y)\n",
    "    10*x^2+y^2\n",
    "end\n",
    "\n",
    "function df01(x)\n",
    "    20*x\n",
    "end\n",
    "\n",
    "function df02(y)\n",
    "    2*y\n",
    "end\n",
    "\n",
    "xs = Float64[]\n",
    "x0=1\n",
    "y0=1\n",
    "\n",
    "xs=zeros(10)\n",
    "ys=zeros(10)\n",
    "\n",
    "for iter in 1:10\n",
    "       xs[iter] = x0;\n",
    "       ys[iter] = y0;\n",
    "       x0   =  x0 - 0.1 * df01(x0)\n",
    "       y0   =  y0 - 0.1 * df02(y0)\n",
    "end\n",
    "\n",
    "plot(\n",
    "    layer(z=(x,y) -> 10*x^2+y^2, x=linspace(-2,2,30), y=linspace(-2,2,30), Geom.contour),\n",
    "    layer(x=xs,y=ys,Geom.point()),\n",
    "    layer(x=xs[1:2],y=ys[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[2:3],y=ys[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[3:4],y=ys[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[4:5],y=ys[4:5],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[5:6],y=ys[5:6],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[6:7],y=ys[6:7],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[7:8],y=ys[7:8],Geom.line(),Theme(default_color=colorant\"orange\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Hessian has a poor condition number, gradient descent performs poorly, as it is confused between one direction where the gradient increases significantly, and another direction where it increases slowly. Gradient descent is unaware of this change in the derivative, so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton’s method**\n",
    "\n",
    "This can be resolved by using information from the Hessian matrix to guide the search. Simplest method is **Newton's method** based on second degree Taylor expansion.\n",
    "\n",
    "$f(\\mathbf{x}) = f(\\mathbf{x^{(0)}}) + (\\mathbf{x}-\\mathbf{x^{(0)}})^T\\nabla_xf(\\mathbf{x^{(0)}}) + \\frac{1}{2}(\\mathbf{x}-\\mathbf{x^{(0)}})^T\\mathbf{H}(f)(\\mathbf{x^{(0)}})(\\mathbf{x}-\\mathbf{x^{(0)}})$\n",
    "\n",
    "Taking gradient wrt **x** and setting L.H.S. to zero:\n",
    "\n",
    "$0 = \\nabla_xf(\\mathbf{x^{(0)}}) + \\mathbf{H}(f)(\\mathbf{x^{(0)}})\\mathbf{x} - \\mathbf{H}(f)(\\mathbf{x^{(0)}})\\mathbf{x^{(0)}}$\n",
    "\n",
    "$\\mathbf{x} = \\mathbf{x^{(0)}} - \\mathbf{H}(f)(\\mathbf{x^{(0)}})^{-1}\\nabla_xf(\\mathbf{x^{(0)}})$\n",
    "\n",
    "This method consists of iteratively jumping to the minimum of a locally approximated quadratic function $\\rightarrow$ converges faster than gradient descent. However, unlike gradient descent, solution of Newton's method is attracted to saddle points as well.\n",
    "\n",
    "**Algorithms**\n",
    "\n",
    "* **first-order optimization algorithms** that use only the gradient, such as gradient descent\n",
    "* **second-order optimization algorithms** that also use the Hessian matrix, such as Newton’s method\n",
    "\n",
    "\n",
    "To treat functions in deep learning, we assume that they are Lipschitz continuous or have lipschitz continuous derivatives. (weak constraint) A **Lipschitz continuous** function satisfies for a Lipschitz constant $\\mathcal{L}$ the bound:\n",
    "\n",
    "$\\forall \\mathbf{x}\\, ,\\forall \\mathbf{y}\\, ,|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq \\mathcal{L}||\\mathbf{x}-\\mathbf{y}||_2$\n",
    "\n",
    "This property is useful because it enables us to quantify our assumption that a small change in the input made by an algorithm such as gradient descent will have a small change in the output.\n",
    "\n",
    "**Convex optimization** algorithms are able to provide many more guarantees by making stronger restrictions. These algorithms are applicable only to convex functions—functions for which the Hessian is positive semideﬁnite everywhere. It is sometimes used as a subroutine in deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(x, y)\n",
    "    10*x^2+y^2\n",
    "end\n",
    "\n",
    "function df0(x,y)\n",
    "    [20*x 2*y]'\n",
    "end\n",
    "\n",
    "xs = Float64[]\n",
    "x0=[1 1]'\n",
    "\n",
    "xs=zeros(10)\n",
    "ys=zeros(10);\n",
    "\n",
    "for iter in 1:10\n",
    "       xs[iter] = x0[1];\n",
    "       ys[iter] = x0[1];\n",
    "       x0   =  x0 - 0.1 * inv([[10 0];[0 1]])*df0(x0[1],x0[2])\n",
    "end\n",
    "\n",
    "plot(\n",
    "    layer(z=(x,y) -> 10*x^2+y^2, x=linspace(-2,2,30), y=linspace(-2,2,30), Geom.contour),\n",
    "    layer(x=xs,y=ys,Geom.point()),\n",
    "    layer(x=xs[1:2],y=ys[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[2:3],y=ys[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[3:4],y=ys[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[4:5],y=ys[4:5],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[5:6],y=ys[5:6],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[6:7],y=ys[6:7],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "    layer(x=xs[7:8],y=ys[7:8],Geom.line(),Theme(default_color=colorant\"orange\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function df1(x, y, b1)\n",
    "sum(- (y - x*b1)' * x)\n",
    "end\n",
    "\n",
    "function df2(x)\n",
    "    sum(x.^2)\n",
    "end\n",
    "\n",
    "b1    = 0.0\n",
    "bold1 = 1.0\n",
    "bvals1 = Float64[]\n",
    "\n",
    "while (sum(abs(b1 - bold1)) > 1e-10)\n",
    "push!(bvals1, b1)\n",
    "       bold1 = b1\n",
    "       der1 = df1(x, y, bold1)\n",
    "       der2 = df2(x)\n",
    "       b1   =  b1 - 0.3 * der1/der2\n",
    "end\n",
    "\n",
    "plot(Guide.title(\"True beta: 2, est. beta: \" * string(b1)),\n",
    "layer(x=x, y=y, Geom.point(), Theme(default_color=colorant\"orange\")),\n",
    "layer(x=x,y=x*bvals1[3], Geom.line(), Theme(default_color=colorant\"red\")),\n",
    "layer(x=x,y=x*bvals1[4], Geom.line(), Theme(default_color=colorant\"black\")),\n",
    "layer(x=x,y=x*bvals1[6], Geom.line(), Theme(default_color=colorant\"green\")),\n",
    "layer(x=x,y=x*bvals1[10], Geom.line(), Theme(default_color=colorant\"red\")),\n",
    "layer(x=x,y=x*b, Geom.line()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(x'x)x'y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "k=zeros(21)\n",
    "l=zeros(21)\n",
    "for a in 1:0.1:3\n",
    "    i=i+1\n",
    "    k[i]=a\n",
    "    l[i]=sum((y - x*a).^2)\n",
    "end\n",
    "\n",
    "i=0\n",
    "kk=zeros(10)\n",
    "lgd=zeros(10)\n",
    "for c in 2:11\n",
    "    i=i+1\n",
    "    kk[i]=bvals1[c]\n",
    "    lgd[i]=sum((y - x*bvals1[c]).^2)\n",
    "end\n",
    "plot(\n",
    "layer(x=k, y=l, Geom.line()),\n",
    "layer(x=kk,y=lgd,Geom.point()),\n",
    "layer(x=kk[1:2],y=lgd[1:2],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=kk[2:3],y=lgd[2:3],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=kk[3:4],y=lgd[3:4],Geom.line(),Theme(default_color=colorant\"orange\")),\n",
    "layer(x=kk[4:5],y=lgd[4:5],Geom.line(),Theme(default_color=colorant\"orange\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Optimization\n",
    "\n",
    "It might be the case that although we want to maximize (or minimize) $f(x)$, but aren't allowed to use all possible values of $x$, say $x \\in \\mathbb{S}$, for some set $\\mathbb{S}$. This now becomes a problem of **Constrained Optimization**. The points $\\mathbf{x}$ in $S$ are called **feasible points**. \n",
    "\n",
    "An example of such a constraint can be the L2-norm constraint, e.g. $|| \\hspace{.1cm} x \\hspace{.1cm}||^2 < 1$. This is useful as we often want the values for our weights to be small (i.e. close to $0$).\n",
    "\n",
    "*Approach*: Design a separate, unconstrained optimization problem, whose solution can be converted to the original constrained optimization problem. E.g. in the above described constrained optimization problem, we could instead minimize:\n",
    "$$g(\\theta) = f([\\cos\\theta, \\sin\\theta]^T)$$\n",
    "\n",
    "with respect to $\\theta$ and return ($\\cos\\theta, \\sin\\theta$).\n",
    "\n",
    "\n",
    "General solution: **Karush–Kuhn–Tucker(KKT)** approach which introduces a **generalized Lagrangian**.\n",
    "\n",
    "Approach: \n",
    "\n",
    "We use $m$ functions $g^{(i)}(x)$ and $n$ functions $h^{(j)}(x)$ to describe $\\mathbb{S}$,  such that any element $x \\in \\mathbb{S}$ satisfies: \n",
    "$$g^{(i)}(x) = 0 \\hspace{.1cm} \\text{and} \\hspace{.1cm} h^{(j)}(x) \\leq 0 \\hspace{.1cm} \\forall \\hspace{.1cm} i, j$$\n",
    "\n",
    "There are two constraints specified here. I'll explain them with an example. Let's take $g(x)$ as $x - 2$ and $h(x)$ as $x-3$. <br>\n",
    "Then for $x = 2$, we have the following:\n",
    "\n",
    "- **Equality constraints**: $g^{(i)}(x) = 0$. Here, $g(2) = 0$. Hence, $x = 2$ satisfies the equality constraints.\n",
    "- **Inequality constraints**: $h^{(i)}(x) \\leq 0$. Here, $h(2) = -1 < 0$. Hence, $x = 2$ satisfies the inequality constraints.\n",
    "\n",
    "Note that for $x = 3$, $h(x)$ is an equality constraint that it satisfies whereas $g(x)$ is neither.\n",
    "\n",
    "New paramaters (called KKT multipliers): $\\lambda_i$, $\\alpha_j$ for each constraint.  <br>\n",
    "Generalized Lagrangian:\n",
    "\n",
    "\n",
    "![lagrangian](images/Lagrangian.png)\n",
    "\n",
    "Now, let: $Y =\\max\\limits_{\\alpha} \\max\\limits_{\\lambda} L(x, \\lambda, \\alpha)$\n",
    "Then, $\\min\\limits_x(f(x)) = \\min\\limits_x(Y)$\n",
    "\n",
    "This is because, if the constraints are satisfied, $Y = f(x)$ and if it isn't, $Y = \\infty$. This ensures that only feasible points are optimal. For finding the maximum of f(x), we can use the same generalized Lagrangian applied on $-f(x)$. \n",
    "\n",
    "The inequality constraints need to be observed more closely. Suppose the optimal point comes out to be $x^*$. If $h^{(i)}(x^*) = 0$, then the constraint is said to be **active**. However, if the constraint is inactive, i.e. $h^{(i)}(x^*) < 0$, then even if we remove the constraint, $x^*$ continues to be a local solution. Also, by definition, an inactive $h^{(i)}$ is negative and hence $\\max\\limits_{\\alpha} \\max\\limits_{\\lambda} L(x, \\lambda, \\alpha) \\Rightarrow \\alpha_i = 0$. Thus, either $\\alpha_i = 0$ or $h^{(i)}(x^*) = 0$ (in the case of active constraint). Hence, $\\mathbf{\\alpha} \\odot h{(x)} = 0$.\n",
    "\n",
    "Intuition: \n",
    "\n",
    "The relation of the optimal point can satisfy only of these two conditions:\n",
    "\n",
    "- The point is at the boundary of the constraint (i.e. active), then the corresponding KKT multiplier should be used.\n",
    "\n",
    "- The constraint has no influence in the evaluation of the point and hence, the corresponding KKT multiplier is zeroed out.\n",
    "\n",
    "The optimal points satisfy the following KKT conditions, which are necessary but not always sufficient:\n",
    "\n",
    "![kkt](images/kkt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lukas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Hidden Units\n",
    "\n",
    "- choosing types of hidden units is difficult, the theory behind it is still being developped\n",
    "- the choice is often made by trial and error\n",
    "- general recommendation: use rectified linear units\n",
    "- most units perform a nonlinear transformation on an affine transformation of inputs: $$g(\\mathbf{z}) = g(\\mathbf{W^T}\\mathbf{x} + \\mathbf{b})$$\n",
    "- the units usually differ only in the choice of *g*\n",
    "- using functions differentiable everywhere is good for gradient-based search of optimal parameters of the network, but often it is sufficient that the function is differentiable almost everywhere (because the gradient-based learning algorithm doesn't usually converge precisely at the local/global minima, but only close to it)\n",
    "- if the function has left and right derivatives defined, most implementations of learning algorithms will work with one of the one-sided derivatives instead of raising an error\n",
    "- computations are subject to numerical errors anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 Rectified Linear Units and Their Generalizations\n",
    "\n",
    "### Rectified Linear Unit\n",
    "\n",
    "- $$g(z) = max(\\{0, z\\})$$\n",
    "- whenever the unit is active, the derivative is *1* which leads to easier gradient-based optimization\n",
    "- a good practice is to initialize *b* with small positive values which increases the chances that the units will be active at the beginning of optimization\n",
    "\n",
    "- when rectified linear units aren't active, the algorithm doesn't learn, hence many generalizations aim at introducing non-zero gradients almost everywhere\n",
    "- most generalizations of rectified linear units perform comparably and perform better only in specific applications\n",
    "\n",
    "### Absolute Value Rectification\n",
    "\n",
    "- $$g(z) = \\mid z\\mid$$\n",
    "- used e.g. for recognition of objects on images\n",
    "\n",
    "### Leaky Rectified Linear Unit\n",
    "\n",
    "- $$g(z) = max(\\{0, z\\}) + \\alpha * min(\\{0, z\\}), \\alpha \\neq 0$$\n",
    "- &\\alpha& set to a small positive value\n",
    "\n",
    "### Parametric Rectified Linear Unit\n",
    "\n",
    "- $$g(z) = max(\\{0, z\\}) + \\alpha * min(\\{0, z\\}), \\alpha \\neq 0$$\n",
    "- the value of $\\alpha$ is optimized by the learning algorithm\n",
    "\n",
    "### Maxout Units\n",
    "\n",
    "- $$h_i(z) = max_{j \\in G^{(i)}}z_{ij})$$\n",
    "- a maxout unit can learn to approximate any convex function with arbitrary precision\n",
    "- contains more parameters and hence requires more regularization\n",
    "- helps avoid *catastrophic forgetting*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "\n",
    "### Logistic Sigmoid\n",
    "\n",
    "- $$g(z) = \\sigma(z)$$\n",
    "\n",
    "### Hyperbolic Tangent\n",
    "\n",
    "- $$g(z) = tanh(z)$$\n",
    "\n",
    "- when a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid because it is more close to the identity function and hence the training is easier\n",
    "\n",
    "- sigmoidal activation functions are more common in settings other than feedforward networks because some of the additional requirements rule out the use of piecewise linear activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 Other Hidden Units\n",
    "\n",
    "- hidden unit design remains an active area of research and many useful hidden unit types remain to be discovered\n",
    "- other types of hidden units are possible, but are used less frequently\n",
    "- many unpublished activation functions perform just as well as the popular ones\n",
    "- usually new hidden unit types are published only if they are clearly demonstrated to provide a significant improvement\n",
    "\n",
    "### Identity\n",
    "\n",
    "### Linear Functions\n",
    "\n",
    "- if only linear transformations are used, the whole network becomes a linear model\n",
    "- linear functions can help decrease the overall number of parameters\n",
    "\n",
    "### Softmax Unit\n",
    "\n",
    "- may be used when we want the layer to output a categorical variable\n",
    "\n",
    "### Radial Basis Function\n",
    "\n",
    "- because it saturates to 0 for most *x*, it can be difficult to optimize\n",
    "\n",
    "### Soflplus\n",
    "\n",
    "- a smooth version of the rectifier\n",
    "- the use of the softplus is generally discouraged\n",
    "- one might expect it to have an advantage over the rectifier due to being differentiable everywhere or due to saturating less completely, but empirically it does not\n",
    "\n",
    "### Hard Tanh\n",
    "\n",
    "- this is shaped similarly to the tanh and the rectifier but unlike the latter, it is bounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Architecture Design\n",
    "\n",
    "- we need to consider the depth of the network and the width of each layer\n",
    "- most neural network architectures arrange these layers in a chain structure, with each layer being a function of the layer that preceded it\n",
    "- a network with even one hidden layer is sufficient to fit the training set\n",
    "- deeper networks often are able to use far fewer units per layer and far fewer parameters and often generalize to the test set, but are also often harder to optimize\n",
    "- the ideal network architecture for a task must be found via experimentation guided by monitoring the validation set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.1 Universal Approximation Properties and Depth\n",
    "\n",
    "- the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any 'squashing' activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function (any continuous function on a closed and bounded subset of $R^n$ is Borel measurable) from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units\n",
    "- universal approximation theorems have also been proved for a wider class of activation functions, which includes the now commonly used rectified linear unit\n",
    "- however, we are not guaranteed that the training algorithm will be able to learn the given function\n",
    "- learning can fail for two different reasons\n",
    "    - the optimization algorithm used for training may not be able to find the value of the parameters that corresponds to the desired function\n",
    "    - the training algorithm might choose the wrong function due to overfitting\n",
    "- the problem is that the network which describes the given function can be very large\n",
    "- in many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error\n",
    "- functions representable with a deep rectifier net can require an exponential number of hidden units with a shallow (one hidden layer) network\n",
    "-  piecewise linear networks (which can be obtained from rectifier nonlinearities or maxout units) can represent functions with a number of regions that is exponential in the depth of the network\n",
    "- choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions\n",
    "- empirically, greater depth does seem to result in better generalization for a wide variety of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2 Other Architectural Considerations\n",
    "\n",
    "- many neural network architectures have been developed for specific tasks\n",
    "- convolutional networks\n",
    "- recurrent neural networks\n",
    "- skipping layers\n",
    "- each unit in the input layer can be connected to only a small subset of units in the output layer (results in lower number of parameters)\n",
    "- the choice of a specific architecture is often problem-dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Back-Propagation and Other Differentiation Algorithms\n",
    "\n",
    "Forward propagation vs. backpropagation\n",
    "- Forward propagation calculates the fitted values and cost function value.\n",
    "- Backpropagation is a method for computing gradient\n",
    "\n",
    "Computational graph\n",
    "- nodes are variables\n",
    "- operation is a function of 1 or more variables, returning only 1 variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need Chain rule\n",
    "\n",
    "$\\mathbb{R}$ $n$ times denoted as $\\mathbb{R}^n$\n",
    "\n",
    "Suppose that $y = g(x)$ and $z = f(g(x)) = f(y)$. Then the chain rule states that\n",
    "$$ \\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} $$\n",
    "\n",
    "Generalized version:\n",
    "Suppose that $x \\in \\mathbb{R}^m$, $y \\in \\mathbb{R}^n$, *g* maps from $\\mathbb{R}^m$ to $\\mathbb{R}^n$ and *f* maps from $\\mathbb{R}^n$ to $\\mathbb{R}$. If ***y*** = *g(**x**)* and *z = f(**y**)*, then\n",
    "$$ \\frac{\\partial z}{\\partial x_i} = \\sum_j \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} $$\n",
    "and in vector notation\n",
    "$$\\nabla \\pmb{x}^z = (\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}})^T \\nabla \\pmb{y}^z$$\n",
    "\n",
    "where $\\frac{\\partial \\pmb{y}}{\\partial \\pmb{x}}$ is the n × m Jacobian matrix of g .\n",
    "\n",
    "\n",
    "The gradient of a variable **x** can be obtained by multiplying a Jacobian by a $\\nabla \\pmb{y}^z$.\n",
    "\n",
    "This can be extended from vectors and matrices to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation algorithm\n",
    "**Algorithm 6.3**\n",
    "\n",
    "Require:\n",
    "- $l$: Network depth\n",
    "- $W^{(i)}, i \\in \\{1,...l\\}$: weight matrices of the model\n",
    "- $b^{(i)}, i \\in \\{1,...l\\}$: bias vectors of the model\n",
    "- $\\pmb{x}$: input vector\n",
    "- $\\pmb{y}$: target output\n",
    "\n",
    "$\\pmb{h^{(0)}} = \\pmb{x}$\n",
    "\n",
    "**for** $k = 1,...l$ **do**\n",
    "\n",
    "$~~~~~~~~\\pmb{a}^{(k)} = \\pmb{b}^{(k)} + \\pmb{W}^{(k)}\\pmb{h}^{(k-1)}$\n",
    "\n",
    "$~~~~~~~~\\pmb{h}^{(k)} = f(\\pmb{a}^{(k)})$\n",
    "\n",
    "**end for**\n",
    "\n",
    "$\\pmb{\\hat{y}} = \\pmb{h}^{(l)}$\n",
    "\n",
    "$J = L(\\pmb{\\hat{y}}, \\pmb{y}) + \\lambda \\Omega(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm\n",
    "Algorithm 6.4\n",
    "\n",
    "- **Back**propagation because it starts from the back (output) and moves towards the input.\n",
    "- computational complexity ~ $O(n)$\n",
    "- In addition to variables in algorithm 6.3, also the output $\\pmb{y}$ is required.\n",
    "- This algorithm yields gradients on the activations.\n",
    "- Interpretation of gradients is that they indicate how each layer's output should change to reduce error.\n",
    "- Gradients on weights and biases are use for weight and bias updates.\n",
    "\n",
    "After the forward computation, compute the gradient on the output layer:\n",
    "\n",
    "$\\pmb{g} \\leftarrow \\nabla_{\\hat{\\pmb{y}}} J = \\nabla_{\\hat{\\pmb{y}}} L(\\pmb{\\hat{y}}, \\pmb{y})$\n",
    "\n",
    "**for** *k = l, l-1,... 1* **do**\n",
    "\n",
    "    Convert the gradients on the layer's output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\n",
    "\n",
    "$~~~~~~~~\\pmb{g} \\leftarrow \\nabla_{\\pmb{a}^{(k)}} J = \\pmb{g} \\bigodot f'(\\pmb{a}^{(k)}) $\n",
    "\n",
    "    Compute gradients on weights and biases (including the reqularization term, where needed):\n",
    "    \n",
    "$~~~~~~~~\\nabla_{\\pmb{b}^{(k)}} J = \\pmb{g} + \\lambda \\nabla_{\\pmb{b}^{(k)}} \\Omega(\\theta)$\n",
    "    \n",
    "$~~~~~~~~\\nabla_{\\pmb{W}^{(k)}} J = \\pmb{g}\\pmb{h}^{(k-1)T} + \\lambda \\nabla_{\\pmb{W}^{(k)}} \\Omega(\\theta)$\n",
    "\n",
    "    Propagate the gradients w.r.t. the next lower-level hidden layer's activations:\n",
    "\n",
    "$~~~~~~~~\\pmb{g} \\leftarrow \\nabla_{\\pmb{h}^{(k-1)}} J = \\pmb{W}^{(k)T}\\pmb{g} $\n",
    "\n",
    "**end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
